{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTOptimizer, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig, AutoQuantizationConfig\n",
    "from transformers import AutoTokenizer, Pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "cache_dir = \"onnx_cache\"\n",
    "NUM_DOCS=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Onnx Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "/Users/sid/miniconda3/envs/speaker_emb/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "\n",
      "***** Exporting submodel 1/1: BertModel *****\n",
      "Using framework PyTorch: 2.2.1\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/Users/sid/miniconda3/envs/speaker_emb/lib/python3.9/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Optimizing model...\n",
      "2024-08-25 11:40:01.365701 [W:onnxruntime:, inference_session.cc:1732 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n",
      "Configuration saved in onnx_cache/ort_config.json\n",
      "Optimized model saved at: onnx_cache (external data format: False; saved all tensor to one file: True)\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: onnx_cache (external data format: False)\n",
      "Configuration saved in onnx_cache/ort_config.json\n",
      "The ONNX file model_optimized_quantized.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_and_optimize_model(model_name, cache_dir):\n",
    "    cache_dir = Path(cache_dir)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load vanilla transformers and convert to onnx\n",
    "    model = ORTModelForFeatureExtraction.from_pretrained(model_name, from_transformers=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Save onnx checkpoint and tokenizer\n",
    "    model.save_pretrained(cache_dir)\n",
    "    tokenizer.save_pretrained(cache_dir)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer = ORTOptimizer.from_pretrained(model)\n",
    "    optimization_config = OptimizationConfig(optimization_level=99)\n",
    "    \n",
    "    optimizer.optimize(\n",
    "        save_dir=cache_dir,\n",
    "        optimization_config=optimization_config,\n",
    "    )\n",
    "    \n",
    "    # Load optimized model\n",
    "    optimized_model = ORTModelForFeatureExtraction.from_pretrained(cache_dir, file_name=\"model_optimized.onnx\")\n",
    "    \n",
    "    # Quantize the model\n",
    "    dynamic_quantizer = ORTQuantizer.from_pretrained(optimized_model)\n",
    "    dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "    \n",
    "    quantized_model_path = dynamic_quantizer.quantize(\n",
    "        save_dir=cache_dir,\n",
    "        quantization_config=dqconfig,\n",
    "    )\n",
    "    \n",
    "    # Load quantized model\n",
    "    quantized_model = ORTModelForFeatureExtraction.from_pretrained(cache_dir, file_name=\"model_optimized_quantized.onnx\")\n",
    "    \n",
    "    return optimized_model, quantized_model, tokenizer\n",
    "\n",
    "optimized_model, quantized_model, tokenizer = process_and_optimize_model(model_name, cache_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 100/100 [00:21<00:00,  4.73it/s]\n",
      "100%|██████████| 100/100 [00:24<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Model:\n",
      "Mean: 0.2102s, Median: 0.2041s, Std: 0.0229s\n",
      "\n",
      "Quantized Model:\n",
      "Mean: 0.2439s, Median: 0.2258s, Std: 0.0631s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run speed tests\n",
    "opt_mean, opt_median, opt_std = utils.run_speed_test(optimized_model, tokenizer, num_docs=NUM_DOCS)\n",
    "quant_mean, quant_median, quant_std = utils.run_speed_test(quantized_model, tokenizer, num_docs=NUM_DOCS)\n",
    "\n",
    "print(\"Optimized Model:\")\n",
    "print(f\"Mean: {opt_mean:.4f}s, Median: {opt_median:.4f}s, Std: {opt_std:.4f}s\")\n",
    "\n",
    "print(\"\\nQuantized Model:\")\n",
    "print(f\"Mean: {quant_mean:.4f}s, Median: {quant_median:.4f}s, Std: {quant_std:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speaker_emb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
